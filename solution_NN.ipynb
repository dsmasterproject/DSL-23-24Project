{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "development = pd.read_csv(\"./../DSL_Winter_Project_2024/development.csv\")\n",
    "evaluation = pd.read_csv(\"./../DSL_Winter_Project_2024/evaluation.csv\")\n",
    "\n",
    "# Outliers have been known from previous analysis\n",
    "outlier_column_index=[0, 7, 12, 15, 16, 17]\n",
    "columns_to_drop=[]\n",
    "\n",
    "for index in outlier_column_index:\n",
    "    columns_to_drop.append('pmax[%s]' % index)\n",
    "    columns_to_drop.append('negpmax[%s]' % index)\n",
    "    columns_to_drop.append('tmax[%s]' % index)\n",
    "    columns_to_drop.append('area[%s]' % index)\n",
    "    columns_to_drop.append('rms[%s]' % index)\n",
    "\n",
    "dev_df=development.drop(columns=columns_to_drop)\n",
    "eva_df=evaluation.drop(columns=columns_to_drop)\n",
    "eva_df=eva_df.drop(columns=[\"Id\"])\n",
    "\n",
    "X=dev_df.drop(columns=['x', 'y'])\n",
    "y=dev_df.loc[:,['x', 'y']]\n",
    "rs=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7710/7710 [==============================] - 4s 489us/step - loss: 36121.3906 - val_loss: 3326.0532\n",
      "Epoch 2/50\n",
      "7710/7710 [==============================] - 4s 466us/step - loss: 744.3203 - val_loss: 54.5026\n",
      "Epoch 3/50\n",
      "7710/7710 [==============================] - 4s 477us/step - loss: 34.1684 - val_loss: 35.5989\n",
      "Epoch 4/50\n",
      "7710/7710 [==============================] - 4s 485us/step - loss: 26.8506 - val_loss: 28.7888\n",
      "Epoch 5/50\n",
      "7710/7710 [==============================] - 4s 474us/step - loss: 23.5371 - val_loss: 24.8251\n",
      "Epoch 6/50\n",
      "7710/7710 [==============================] - 4s 468us/step - loss: 21.6698 - val_loss: 23.9111\n",
      "Epoch 7/50\n",
      "7710/7710 [==============================] - 4s 466us/step - loss: 20.2351 - val_loss: 23.5579\n",
      "Epoch 8/50\n",
      "7710/7710 [==============================] - 4s 466us/step - loss: 19.0806 - val_loss: 21.7137\n",
      "Epoch 9/50\n",
      "7710/7710 [==============================] - 4s 465us/step - loss: 18.3771 - val_loss: 21.6108\n",
      "Epoch 10/50\n",
      "7710/7710 [==============================] - 4s 465us/step - loss: 17.6943 - val_loss: 20.7856\n",
      "Epoch 11/50\n",
      "7710/7710 [==============================] - 4s 468us/step - loss: 17.2043 - val_loss: 20.3594\n",
      "Epoch 12/50\n",
      "7710/7710 [==============================] - 4s 466us/step - loss: 16.7668 - val_loss: 20.1893\n",
      "Epoch 13/50\n",
      "7710/7710 [==============================] - 4s 476us/step - loss: 16.4306 - val_loss: 19.6719\n",
      "Epoch 14/50\n",
      "7710/7710 [==============================] - 4s 500us/step - loss: 16.1435 - val_loss: 19.5704\n",
      "Epoch 15/50\n",
      "7710/7710 [==============================] - 4s 502us/step - loss: 15.9044 - val_loss: 20.0044\n",
      "Epoch 16/50\n",
      "7710/7710 [==============================] - 4s 471us/step - loss: 15.6199 - val_loss: 19.1419\n",
      "Epoch 17/50\n",
      "7710/7710 [==============================] - 4s 467us/step - loss: 15.3109 - val_loss: 19.1130\n",
      "Epoch 18/50\n",
      "7710/7710 [==============================] - 4s 461us/step - loss: 15.1006 - val_loss: 18.9274\n",
      "Epoch 19/50\n",
      "7710/7710 [==============================] - 4s 472us/step - loss: 14.9444 - val_loss: 18.7063\n",
      "Epoch 20/50\n",
      "7710/7710 [==============================] - 4s 472us/step - loss: 14.6566 - val_loss: 18.5925\n",
      "Epoch 21/50\n",
      "7710/7710 [==============================] - 4s 480us/step - loss: 14.5235 - val_loss: 18.5738\n",
      "Epoch 22/50\n",
      "7710/7710 [==============================] - 4s 474us/step - loss: 14.4071 - val_loss: 19.0363\n",
      "Epoch 23/50\n",
      "7710/7710 [==============================] - 4s 497us/step - loss: 14.1321 - val_loss: 17.9570\n",
      "Epoch 24/50\n",
      "7710/7710 [==============================] - 4s 499us/step - loss: 14.0958 - val_loss: 18.3863\n",
      "Epoch 25/50\n",
      "7710/7710 [==============================] - 4s 498us/step - loss: 13.9940 - val_loss: 18.6172\n",
      "Epoch 26/50\n",
      "7710/7710 [==============================] - 4s 501us/step - loss: 13.8916 - val_loss: 18.7223\n",
      "Epoch 27/50\n",
      "7710/7710 [==============================] - 4s 492us/step - loss: 13.7179 - val_loss: 18.0267\n",
      "Epoch 28/50\n",
      "7710/7710 [==============================] - 4s 473us/step - loss: 13.6841 - val_loss: 18.0281\n",
      "Epoch 29/50\n",
      "7710/7710 [==============================] - 4s 473us/step - loss: 13.6722 - val_loss: 17.9728\n",
      "Epoch 30/50\n",
      "7710/7710 [==============================] - 4s 473us/step - loss: 13.5642 - val_loss: 19.0802\n",
      "Epoch 31/50\n",
      "7710/7710 [==============================] - 4s 491us/step - loss: 13.4745 - val_loss: 18.0003\n",
      "Epoch 32/50\n",
      "7710/7710 [==============================] - 4s 500us/step - loss: 13.3454 - val_loss: 17.8449\n",
      "Epoch 33/50\n",
      "7710/7710 [==============================] - 4s 498us/step - loss: 13.2748 - val_loss: 17.9827\n",
      "Epoch 34/50\n",
      "7710/7710 [==============================] - 4s 488us/step - loss: 13.2354 - val_loss: 18.1135\n",
      "Epoch 35/50\n",
      "7710/7710 [==============================] - 4s 487us/step - loss: 13.1849 - val_loss: 18.1263\n",
      "Epoch 36/50\n",
      "7710/7710 [==============================] - 4s 492us/step - loss: 13.0857 - val_loss: 17.4032\n",
      "Epoch 37/50\n",
      "7710/7710 [==============================] - 4s 480us/step - loss: 13.0644 - val_loss: 17.4300\n",
      "Epoch 38/50\n",
      "7710/7710 [==============================] - 4s 480us/step - loss: 12.9433 - val_loss: 17.5418\n",
      "Epoch 39/50\n",
      "7710/7710 [==============================] - 4s 481us/step - loss: 12.9335 - val_loss: 18.0020\n",
      "Epoch 40/50\n",
      "7710/7710 [==============================] - 4s 489us/step - loss: 12.8143 - val_loss: 17.8487\n",
      "Epoch 41/50\n",
      "7710/7710 [==============================] - 4s 484us/step - loss: 12.8829 - val_loss: 18.4897\n",
      "Epoch 42/50\n",
      "7710/7710 [==============================] - 4s 472us/step - loss: 12.6896 - val_loss: 17.9726\n",
      "Epoch 43/50\n",
      "7710/7710 [==============================] - 4s 494us/step - loss: 12.7261 - val_loss: 18.1095\n",
      "Epoch 44/50\n",
      "7710/7710 [==============================] - 4s 463us/step - loss: 12.6822 - val_loss: 18.1289\n",
      "Epoch 45/50\n",
      "7710/7710 [==============================] - 4s 460us/step - loss: 12.5728 - val_loss: 17.6661\n",
      "Epoch 46/50\n",
      "7710/7710 [==============================] - 4s 469us/step - loss: 12.5588 - val_loss: 17.5613\n",
      "Epoch 47/50\n",
      "7710/7710 [==============================] - 4s 474us/step - loss: 12.5741 - val_loss: 17.7652\n",
      "Epoch 48/50\n",
      "7710/7710 [==============================] - 4s 498us/step - loss: 12.4725 - val_loss: 17.7241\n",
      "Epoch 49/50\n",
      "7710/7710 [==============================] - 4s 472us/step - loss: 12.4022 - val_loss: 17.5913\n",
      "Epoch 50/50\n",
      "7710/7710 [==============================] - 4s 468us/step - loss: 12.4088 - val_loss: 17.2829\n",
      "2410/2410 [==============================] - 1s 245us/step\n",
      "4.2332971267991395\n"
     ]
    }
   ],
   "source": [
    "# normalized data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# set the parameters to normal values to see the model effect.\n",
    "def build_model(hidden_units=64, activation='relu', optimizer='adam'):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(hidden_units, activation=activation, input_shape=(X_train.shape[1],)),\n",
    "        layers.Dense(hidden_units, activation=activation),\n",
    "        layers.Dense(2)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(hidden_units=128, activation='sigmoid', optimizer='adam')\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "distance=np.mean(np.sqrt(np.sum((y_pred - y_test) ** 2, axis=1)))\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7710/7710 [==============================] - 4s 506us/step - loss: 36518.9766 - mae: 140.9164 - acc: 0.6997 - val_loss: 3259.9126 - val_mae: 41.3758 - val_acc: 0.9814\n",
      "Epoch 2/50\n",
      "7710/7710 [==============================] - 4s 486us/step - loss: 735.8427 - mae: 14.1608 - acc: 0.9852 - val_loss: 56.2554 - val_mae: 4.3699 - val_acc: 0.9864\n",
      "Epoch 3/50\n",
      "7710/7710 [==============================] - 4s 470us/step - loss: 35.2692 - mae: 3.8106 - acc: 0.9864 - val_loss: 37.2252 - val_mae: 3.5812 - val_acc: 0.9873\n",
      "Epoch 4/50\n",
      "7710/7710 [==============================] - 4s 465us/step - loss: 27.7881 - mae: 3.4778 - acc: 0.9871 - val_loss: 30.9284 - val_mae: 3.3801 - val_acc: 0.9878\n",
      "Epoch 5/50\n",
      "7710/7710 [==============================] - 4s 472us/step - loss: 24.6439 - mae: 3.3287 - acc: 0.9875 - val_loss: 27.9025 - val_mae: 3.3196 - val_acc: 0.9871\n",
      "Epoch 6/50\n",
      "7710/7710 [==============================] - 4s 475us/step - loss: 22.6843 - mae: 3.2229 - acc: 0.9876 - val_loss: 25.3090 - val_mae: 3.1928 - val_acc: 0.9888\n",
      "Epoch 7/50\n",
      "7710/7710 [==============================] - 4s 469us/step - loss: 21.4809 - mae: 3.1487 - acc: 0.9881 - val_loss: 25.1620 - val_mae: 3.1372 - val_acc: 0.9886\n",
      "Epoch 8/50\n",
      "7710/7710 [==============================] - 4s 470us/step - loss: 20.1940 - mae: 3.0885 - acc: 0.9883 - val_loss: 22.9248 - val_mae: 3.0696 - val_acc: 0.9885\n",
      "Epoch 9/50\n",
      "7710/7710 [==============================] - 4s 469us/step - loss: 19.2390 - mae: 3.0375 - acc: 0.9885 - val_loss: 22.9496 - val_mae: 3.0485 - val_acc: 0.9893\n",
      "Epoch 10/50\n",
      "7710/7710 [==============================] - 4s 499us/step - loss: 18.3992 - mae: 2.9948 - acc: 0.9886 - val_loss: 23.2714 - val_mae: 3.0718 - val_acc: 0.9896\n",
      "Epoch 11/50\n",
      "7710/7710 [==============================] - 4s 499us/step - loss: 17.8212 - mae: 2.9642 - acc: 0.9886 - val_loss: 22.5359 - val_mae: 2.9762 - val_acc: 0.9895\n",
      "Epoch 12/50\n",
      "7710/7710 [==============================] - 4s 488us/step - loss: 17.3594 - mae: 2.9361 - acc: 0.9888 - val_loss: 22.1453 - val_mae: 2.9774 - val_acc: 0.9877\n",
      "Epoch 13/50\n",
      "7710/7710 [==============================] - 4s 477us/step - loss: 16.9767 - mae: 2.9111 - acc: 0.9886 - val_loss: 21.3873 - val_mae: 2.9450 - val_acc: 0.9890\n",
      "Epoch 14/50\n",
      "7710/7710 [==============================] - 4s 523us/step - loss: 16.6274 - mae: 2.8897 - acc: 0.9889 - val_loss: 20.2691 - val_mae: 2.8860 - val_acc: 0.9886\n",
      "Epoch 15/50\n",
      "7710/7710 [==============================] - 4s 511us/step - loss: 16.2863 - mae: 2.8718 - acc: 0.9888 - val_loss: 20.2784 - val_mae: 2.8856 - val_acc: 0.9886\n",
      "Epoch 16/50\n",
      "7710/7710 [==============================] - 4s 472us/step - loss: 15.9664 - mae: 2.8522 - acc: 0.9891 - val_loss: 20.7550 - val_mae: 2.9183 - val_acc: 0.9895\n",
      "Epoch 17/50\n",
      "7710/7710 [==============================] - 4s 473us/step - loss: 15.7608 - mae: 2.8395 - acc: 0.9890 - val_loss: 19.6317 - val_mae: 2.8604 - val_acc: 0.9888\n",
      "Epoch 18/50\n",
      "7710/7710 [==============================] - 4s 473us/step - loss: 15.4761 - mae: 2.8261 - acc: 0.9890 - val_loss: 19.2152 - val_mae: 2.8488 - val_acc: 0.9891\n",
      "Epoch 19/50\n",
      "7710/7710 [==============================] - 4s 478us/step - loss: 15.3405 - mae: 2.8112 - acc: 0.9894 - val_loss: 19.5089 - val_mae: 2.8803 - val_acc: 0.9899\n",
      "Epoch 20/50\n",
      "7710/7710 [==============================] - 4s 486us/step - loss: 15.1284 - mae: 2.7978 - acc: 0.9891 - val_loss: 18.9410 - val_mae: 2.8534 - val_acc: 0.9896\n",
      "Epoch 21/50\n",
      "7710/7710 [==============================] - 4s 485us/step - loss: 14.9348 - mae: 2.7872 - acc: 0.9892 - val_loss: 18.7632 - val_mae: 2.8252 - val_acc: 0.9899\n",
      "Epoch 22/50\n",
      "7710/7710 [==============================] - 4s 475us/step - loss: 14.8229 - mae: 2.7781 - acc: 0.9891 - val_loss: 18.7793 - val_mae: 2.8094 - val_acc: 0.9879\n",
      "Epoch 23/50\n",
      "7710/7710 [==============================] - 4s 472us/step - loss: 14.6227 - mae: 2.7663 - acc: 0.9893 - val_loss: 18.5359 - val_mae: 2.8205 - val_acc: 0.9895\n",
      "Epoch 24/50\n",
      "7710/7710 [==============================] - 4s 472us/step - loss: 14.4626 - mae: 2.7580 - acc: 0.9891 - val_loss: 19.6363 - val_mae: 2.9470 - val_acc: 0.9887\n",
      "Epoch 25/50\n",
      "7710/7710 [==============================] - 4s 473us/step - loss: 14.4010 - mae: 2.7515 - acc: 0.9894 - val_loss: 19.5608 - val_mae: 2.8591 - val_acc: 0.9901\n",
      "Epoch 26/50\n",
      "7710/7710 [==============================] - 4s 472us/step - loss: 14.2629 - mae: 2.7419 - acc: 0.9894 - val_loss: 17.9883 - val_mae: 2.7810 - val_acc: 0.9896\n",
      "Epoch 27/50\n",
      "7710/7710 [==============================] - 4s 477us/step - loss: 14.1844 - mae: 2.7356 - acc: 0.9893 - val_loss: 18.1778 - val_mae: 2.7806 - val_acc: 0.9897\n",
      "Epoch 28/50\n",
      "7710/7710 [==============================] - 4s 483us/step - loss: 13.9778 - mae: 2.7258 - acc: 0.9893 - val_loss: 18.1037 - val_mae: 2.7799 - val_acc: 0.9898\n",
      "Epoch 29/50\n",
      "7710/7710 [==============================] - 4s 505us/step - loss: 13.8566 - mae: 2.7190 - acc: 0.9893 - val_loss: 18.0584 - val_mae: 2.7863 - val_acc: 0.9884\n",
      "Epoch 30/50\n",
      "7710/7710 [==============================] - 4s 488us/step - loss: 13.8155 - mae: 2.7147 - acc: 0.9894 - val_loss: 18.5270 - val_mae: 2.7872 - val_acc: 0.9896\n",
      "Epoch 31/50\n",
      "7710/7710 [==============================] - 4s 497us/step - loss: 13.6580 - mae: 2.7057 - acc: 0.9893 - val_loss: 18.3655 - val_mae: 2.8175 - val_acc: 0.9897\n",
      "Epoch 32/50\n",
      "7710/7710 [==============================] - 4s 509us/step - loss: 13.6743 - mae: 2.7009 - acc: 0.9894 - val_loss: 18.1696 - val_mae: 2.7797 - val_acc: 0.9898\n",
      "Epoch 33/50\n",
      "7710/7710 [==============================] - 4s 493us/step - loss: 13.6107 - mae: 2.6925 - acc: 0.9894 - val_loss: 17.8189 - val_mae: 2.7439 - val_acc: 0.9901\n",
      "Epoch 34/50\n",
      "7710/7710 [==============================] - 4s 474us/step - loss: 13.4593 - mae: 2.6871 - acc: 0.9894 - val_loss: 17.5726 - val_mae: 2.7167 - val_acc: 0.9898\n",
      "Epoch 35/50\n",
      "7710/7710 [==============================] - 4s 474us/step - loss: 13.3691 - mae: 2.6792 - acc: 0.9894 - val_loss: 18.1935 - val_mae: 2.7372 - val_acc: 0.9899\n",
      "Epoch 36/50\n",
      "7710/7710 [==============================] - 4s 474us/step - loss: 13.3726 - mae: 2.6750 - acc: 0.9893 - val_loss: 17.4072 - val_mae: 2.7354 - val_acc: 0.9896\n",
      "Epoch 37/50\n",
      "7710/7710 [==============================] - 4s 473us/step - loss: 13.1878 - mae: 2.6698 - acc: 0.9896 - val_loss: 17.9062 - val_mae: 2.7458 - val_acc: 0.9892\n",
      "Epoch 38/50\n",
      "7710/7710 [==============================] - 4s 475us/step - loss: 13.1399 - mae: 2.6677 - acc: 0.9894 - val_loss: 17.6638 - val_mae: 2.7392 - val_acc: 0.9897\n",
      "Epoch 39/50\n",
      "7710/7710 [==============================] - 4s 477us/step - loss: 13.1214 - mae: 2.6619 - acc: 0.9896 - val_loss: 18.1724 - val_mae: 2.7595 - val_acc: 0.9900\n",
      "Epoch 40/50\n",
      "7710/7710 [==============================] - 4s 476us/step - loss: 13.0329 - mae: 2.6563 - acc: 0.9894 - val_loss: 18.5555 - val_mae: 2.8077 - val_acc: 0.9891\n",
      "Epoch 41/50\n",
      "7710/7710 [==============================] - 4s 481us/step - loss: 12.9502 - mae: 2.6513 - acc: 0.9896 - val_loss: 17.9533 - val_mae: 2.7171 - val_acc: 0.9898\n",
      "Epoch 42/50\n",
      "7710/7710 [==============================] - 4s 506us/step - loss: 12.9328 - mae: 2.6495 - acc: 0.9896 - val_loss: 17.7984 - val_mae: 2.7236 - val_acc: 0.9894\n",
      "Epoch 43/50\n",
      "7710/7710 [==============================] - 4s 503us/step - loss: 12.8336 - mae: 2.6444 - acc: 0.9896 - val_loss: 17.9212 - val_mae: 2.7600 - val_acc: 0.9893\n",
      "Epoch 44/50\n",
      "7710/7710 [==============================] - 4s 496us/step - loss: 12.7959 - mae: 2.6404 - acc: 0.9897 - val_loss: 17.7224 - val_mae: 2.7134 - val_acc: 0.9895\n",
      "Epoch 45/50\n",
      "7710/7710 [==============================] - 4s 518us/step - loss: 12.7506 - mae: 2.6340 - acc: 0.9896 - val_loss: 17.7234 - val_mae: 2.7231 - val_acc: 0.9900\n",
      "Epoch 46/50\n",
      "7710/7710 [==============================] - 4s 491us/step - loss: 12.7395 - mae: 2.6347 - acc: 0.9896 - val_loss: 17.5421 - val_mae: 2.7265 - val_acc: 0.9908\n",
      "Epoch 47/50\n",
      "7710/7710 [==============================] - 4s 489us/step - loss: 12.6699 - mae: 2.6296 - acc: 0.9896 - val_loss: 17.6335 - val_mae: 2.7027 - val_acc: 0.9907\n",
      "Epoch 48/50\n",
      "7710/7710 [==============================] - 4s 506us/step - loss: 12.5582 - mae: 2.6232 - acc: 0.9895 - val_loss: 17.6396 - val_mae: 2.6955 - val_acc: 0.9890\n",
      "Epoch 49/50\n",
      "7710/7710 [==============================] - 4s 501us/step - loss: 12.5947 - mae: 2.6219 - acc: 0.9896 - val_loss: 17.6387 - val_mae: 2.6976 - val_acc: 0.9903\n",
      "Epoch 50/50\n",
      "7710/7710 [==============================] - 4s 496us/step - loss: 12.5441 - mae: 2.6200 - acc: 0.9897 - val_loss: 17.6468 - val_mae: 2.6818 - val_acc: 0.9903\n",
      "2410/2410 [==============================] - 1s 268us/step\n",
      "4.181203413941417\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def build_model(hidden_units=64, activation='relu', optimizer='adam'):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(hidden_units, activation=activation, input_shape=(X_train.shape[1],)),\n",
    "        layers.Dense(hidden_units, activation=activation),\n",
    "        layers.Dense(2)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(hidden_units=128, activation='sigmoid', optimizer='adam')\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "distance=np.mean(np.sqrt(np.sum((y_pred - y_test) ** 2, axis=1)))\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4016/4016 [==============================] - 1s 256us/step\n"
     ]
    }
   ],
   "source": [
    "eva_df_scaled = scaler.transform(eva_df)\n",
    "\n",
    "eva_y=model.predict(eva_df_scaled)\n",
    "\n",
    "df = pd.DataFrame(eva_y, columns=['Id', 'Predicted'])\n",
    "df['Predicted'] = df.apply(lambda row: f\"{row['Id']:.1f}|{row['Predicted']:.1f}\", axis=1)\n",
    "df['Id'] = df.index\n",
    "df.to_csv('tensor_output1.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MinMaxScaler is not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7710/7710 [==============================] - 4s 513us/step - loss: 42320.2539 - mae: 164.0027 - acc: 0.5200 - val_loss: 13718.3223 - val_mae: 100.4028 - val_acc: 0.5218\n",
      "Epoch 2/50\n",
      "7710/7710 [==============================] - 4s 474us/step - loss: 5580.7534 - mae: 52.1360 - acc: 0.8182 - val_loss: 249.4449 - val_mae: 10.1418 - val_acc: 0.9776\n",
      "Epoch 3/50\n",
      "7710/7710 [==============================] - 4s 470us/step - loss: 82.8928 - mae: 6.2532 - acc: 0.9803 - val_loss: 46.0678 - val_mae: 5.1367 - val_acc: 0.9823\n",
      "Epoch 4/50\n",
      "7710/7710 [==============================] - 4s 478us/step - loss: 40.9770 - mae: 4.8438 - acc: 0.9826 - val_loss: 37.5230 - val_mae: 4.6528 - val_acc: 0.9839\n",
      "Epoch 5/50\n",
      "7710/7710 [==============================] - 4s 467us/step - loss: 36.5249 - mae: 4.5997 - acc: 0.9830 - val_loss: 34.8087 - val_mae: 4.4972 - val_acc: 0.9817\n",
      "Epoch 6/50\n",
      "7710/7710 [==============================] - 4s 465us/step - loss: 34.3203 - mae: 4.4742 - acc: 0.9833 - val_loss: 33.3961 - val_mae: 4.4218 - val_acc: 0.9815\n",
      "Epoch 7/50\n",
      "7710/7710 [==============================] - 4s 466us/step - loss: 32.9617 - mae: 4.3893 - acc: 0.9840 - val_loss: 32.5892 - val_mae: 4.3730 - val_acc: 0.9833\n",
      "Epoch 8/50\n",
      "7710/7710 [==============================] - 4s 466us/step - loss: 31.9580 - mae: 4.3241 - acc: 0.9844 - val_loss: 32.5892 - val_mae: 4.3480 - val_acc: 0.9837\n",
      "Epoch 9/50\n",
      "7710/7710 [==============================] - 4s 466us/step - loss: 31.1842 - mae: 4.2766 - acc: 0.9846 - val_loss: 29.8942 - val_mae: 4.1824 - val_acc: 0.9845\n",
      "Epoch 10/50\n",
      "7710/7710 [==============================] - 4s 466us/step - loss: 30.6332 - mae: 4.2403 - acc: 0.9847 - val_loss: 30.2781 - val_mae: 4.2182 - val_acc: 0.9836\n",
      "Epoch 11/50\n",
      "7710/7710 [==============================] - 4s 467us/step - loss: 30.0495 - mae: 4.2014 - acc: 0.9850 - val_loss: 30.8512 - val_mae: 4.2612 - val_acc: 0.9808\n",
      "Epoch 12/50\n",
      "7710/7710 [==============================] - 4s 478us/step - loss: 29.6194 - mae: 4.1724 - acc: 0.9849 - val_loss: 30.1398 - val_mae: 4.2103 - val_acc: 0.9843\n",
      "Epoch 13/50\n",
      "7710/7710 [==============================] - 4s 484us/step - loss: 29.2571 - mae: 4.1486 - acc: 0.9851 - val_loss: 28.4116 - val_mae: 4.0855 - val_acc: 0.9836\n",
      "Epoch 14/50\n",
      "7710/7710 [==============================] - 4s 495us/step - loss: 28.9049 - mae: 4.1247 - acc: 0.9852 - val_loss: 28.5228 - val_mae: 4.0970 - val_acc: 0.9860\n",
      "Epoch 15/50\n",
      "7710/7710 [==============================] - 4s 486us/step - loss: 28.5587 - mae: 4.1010 - acc: 0.9851 - val_loss: 27.8471 - val_mae: 4.0462 - val_acc: 0.9849\n",
      "Epoch 16/50\n",
      "7710/7710 [==============================] - 4s 470us/step - loss: 28.2686 - mae: 4.0788 - acc: 0.9855 - val_loss: 27.4939 - val_mae: 4.0238 - val_acc: 0.9852\n",
      "Epoch 17/50\n",
      "7710/7710 [==============================] - 4s 472us/step - loss: 28.0044 - mae: 4.0604 - acc: 0.9855 - val_loss: 28.1617 - val_mae: 4.0740 - val_acc: 0.9856\n",
      "Epoch 18/50\n",
      "7710/7710 [==============================] - 4s 471us/step - loss: 27.8067 - mae: 4.0467 - acc: 0.9855 - val_loss: 27.3874 - val_mae: 4.0170 - val_acc: 0.9845\n",
      "Epoch 19/50\n",
      "7710/7710 [==============================] - 4s 471us/step - loss: 27.5303 - mae: 4.0278 - acc: 0.9854 - val_loss: 27.0390 - val_mae: 3.9882 - val_acc: 0.9857\n",
      "Epoch 20/50\n",
      "7710/7710 [==============================] - 4s 473us/step - loss: 27.3366 - mae: 4.0147 - acc: 0.9854 - val_loss: 26.7996 - val_mae: 3.9675 - val_acc: 0.9848\n",
      "Epoch 21/50\n",
      "7710/7710 [==============================] - 4s 472us/step - loss: 27.1328 - mae: 3.9977 - acc: 0.9856 - val_loss: 26.7600 - val_mae: 3.9684 - val_acc: 0.9869\n",
      "Epoch 22/50\n",
      "7710/7710 [==============================] - 4s 499us/step - loss: 26.9853 - mae: 3.9894 - acc: 0.9857 - val_loss: 26.6921 - val_mae: 3.9649 - val_acc: 0.9852\n",
      "Epoch 23/50\n",
      "7710/7710 [==============================] - 4s 485us/step - loss: 26.8010 - mae: 3.9718 - acc: 0.9857 - val_loss: 29.4320 - val_mae: 4.1418 - val_acc: 0.9857\n",
      "Epoch 24/50\n",
      "7710/7710 [==============================] - 4s 473us/step - loss: 26.6483 - mae: 3.9627 - acc: 0.9855 - val_loss: 26.2632 - val_mae: 3.9300 - val_acc: 0.9863\n",
      "Epoch 25/50\n",
      "7710/7710 [==============================] - 4s 470us/step - loss: 26.4722 - mae: 3.9495 - acc: 0.9857 - val_loss: 26.5375 - val_mae: 3.9403 - val_acc: 0.9851\n",
      "Epoch 26/50\n",
      "7710/7710 [==============================] - 4s 472us/step - loss: 26.3751 - mae: 3.9418 - acc: 0.9857 - val_loss: 25.9531 - val_mae: 3.9050 - val_acc: 0.9868\n",
      "Epoch 27/50\n",
      "7710/7710 [==============================] - 4s 473us/step - loss: 26.2293 - mae: 3.9321 - acc: 0.9858 - val_loss: 26.4136 - val_mae: 3.9394 - val_acc: 0.9863\n",
      "Epoch 28/50\n",
      "7710/7710 [==============================] - 4s 473us/step - loss: 26.0822 - mae: 3.9207 - acc: 0.9857 - val_loss: 26.9351 - val_mae: 3.9892 - val_acc: 0.9861\n",
      "Epoch 29/50\n",
      "7710/7710 [==============================] - 4s 470us/step - loss: 25.9317 - mae: 3.9101 - acc: 0.9857 - val_loss: 25.5131 - val_mae: 3.8745 - val_acc: 0.9862\n",
      "Epoch 30/50\n",
      "7710/7710 [==============================] - 4s 472us/step - loss: 25.8380 - mae: 3.9025 - acc: 0.9857 - val_loss: 25.5259 - val_mae: 3.8736 - val_acc: 0.9864\n",
      "Epoch 31/50\n",
      "7710/7710 [==============================] - 4s 473us/step - loss: 25.6875 - mae: 3.8909 - acc: 0.9859 - val_loss: 24.9810 - val_mae: 3.8306 - val_acc: 0.9861\n",
      "Epoch 32/50\n",
      "7710/7710 [==============================] - 4s 481us/step - loss: 25.6111 - mae: 3.8853 - acc: 0.9857 - val_loss: 25.1219 - val_mae: 3.8424 - val_acc: 0.9865\n",
      "Epoch 33/50\n",
      "7710/7710 [==============================] - 4s 496us/step - loss: 25.4894 - mae: 3.8767 - acc: 0.9860 - val_loss: 25.6733 - val_mae: 3.8872 - val_acc: 0.9864\n",
      "Epoch 34/50\n",
      "7710/7710 [==============================] - 4s 493us/step - loss: 25.3904 - mae: 3.8679 - acc: 0.9860 - val_loss: 26.0258 - val_mae: 3.9256 - val_acc: 0.9854\n",
      "Epoch 35/50\n",
      "7710/7710 [==============================] - 4s 482us/step - loss: 25.2891 - mae: 3.8609 - acc: 0.9858 - val_loss: 25.1487 - val_mae: 3.8392 - val_acc: 0.9847\n",
      "Epoch 36/50\n",
      "7710/7710 [==============================] - 4s 481us/step - loss: 25.1425 - mae: 3.8512 - acc: 0.9860 - val_loss: 25.4672 - val_mae: 3.8734 - val_acc: 0.9847\n",
      "Epoch 37/50\n",
      "7710/7710 [==============================] - 4s 482us/step - loss: 25.0727 - mae: 3.8454 - acc: 0.9859 - val_loss: 27.1215 - val_mae: 4.0083 - val_acc: 0.9827\n",
      "Epoch 38/50\n",
      "7710/7710 [==============================] - 4s 481us/step - loss: 24.9540 - mae: 3.8348 - acc: 0.9860 - val_loss: 25.2948 - val_mae: 3.8510 - val_acc: 0.9854\n",
      "Epoch 39/50\n",
      "7710/7710 [==============================] - 4s 481us/step - loss: 24.8797 - mae: 3.8305 - acc: 0.9859 - val_loss: 25.3400 - val_mae: 3.8661 - val_acc: 0.9872\n",
      "Epoch 40/50\n",
      "7710/7710 [==============================] - 4s 481us/step - loss: 24.7564 - mae: 3.8217 - acc: 0.9860 - val_loss: 24.6163 - val_mae: 3.8025 - val_acc: 0.9864\n",
      "Epoch 41/50\n",
      "7710/7710 [==============================] - 4s 484us/step - loss: 24.6619 - mae: 3.8144 - acc: 0.9861 - val_loss: 24.7914 - val_mae: 3.8198 - val_acc: 0.9862\n",
      "Epoch 42/50\n",
      "7710/7710 [==============================] - 4s 486us/step - loss: 24.5901 - mae: 3.8081 - acc: 0.9860 - val_loss: 24.3014 - val_mae: 3.7803 - val_acc: 0.9862\n",
      "Epoch 43/50\n",
      "7710/7710 [==============================] - 4s 486us/step - loss: 24.5105 - mae: 3.8012 - acc: 0.9860 - val_loss: 24.4802 - val_mae: 3.7976 - val_acc: 0.9869\n",
      "Epoch 44/50\n",
      "7710/7710 [==============================] - 4s 498us/step - loss: 24.4210 - mae: 3.7956 - acc: 0.9860 - val_loss: 24.9706 - val_mae: 3.8405 - val_acc: 0.9849\n",
      "Epoch 45/50\n",
      "7710/7710 [==============================] - 4s 490us/step - loss: 24.3434 - mae: 3.7905 - acc: 0.9860 - val_loss: 25.1940 - val_mae: 3.8622 - val_acc: 0.9839\n",
      "Epoch 46/50\n",
      "7710/7710 [==============================] - 4s 490us/step - loss: 24.2537 - mae: 3.7833 - acc: 0.9861 - val_loss: 24.2957 - val_mae: 3.7827 - val_acc: 0.9856\n",
      "Epoch 47/50\n",
      "7710/7710 [==============================] - 4s 488us/step - loss: 24.1840 - mae: 3.7757 - acc: 0.9862 - val_loss: 24.6142 - val_mae: 3.8112 - val_acc: 0.9851\n",
      "Epoch 48/50\n",
      "7710/7710 [==============================] - 4s 482us/step - loss: 24.1022 - mae: 3.7724 - acc: 0.9862 - val_loss: 24.3055 - val_mae: 3.7809 - val_acc: 0.9868\n",
      "Epoch 49/50\n",
      "7710/7710 [==============================] - 4s 483us/step - loss: 24.0615 - mae: 3.7681 - acc: 0.9861 - val_loss: 24.2554 - val_mae: 3.7843 - val_acc: 0.9870\n",
      "Epoch 50/50\n",
      "7710/7710 [==============================] - 4s 481us/step - loss: 24.0201 - mae: 3.7661 - acc: 0.9861 - val_loss: 23.7557 - val_mae: 3.7394 - val_acc: 0.9862\n",
      "2410/2410 [==============================] - 1s 242us/step\n",
      "5.890410082861226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def build_model(hidden_units=64, activation='relu', optimizer='adam'):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(hidden_units, activation=activation, input_shape=(X_train.shape[1],)),\n",
    "        layers.Dense(hidden_units, activation=activation),\n",
    "        layers.Dense(2)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(hidden_units=128, activation='sigmoid', optimizer='adam')\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "distance=np.mean(np.sqrt(np.sum((y_pred - y_test) ** 2, axis=1)))\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pmax and area are two features with linear correlation, but removing them here does not improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmax_features = []\n",
    "area_features = []\n",
    "\n",
    "for i in range(0, 18):\n",
    "    if(i in outlier_column_index):\n",
    "        continue\n",
    "    pmax_features.append(\"pmax[%s]\" % (i))\n",
    "    area_features.append(\"area[%s]\" % (i))\n",
    "\n",
    "X_train_drop_pmax = X_train.drop(columns=pmax_features + area_features)\n",
    "X_test_drop_pmax=X_test.drop(columns=pmax_features + area_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7710/7710 [==============================] - 5s 618us/step - loss: 37451.3320 - mae: 145.4534 - acc: 0.6329 - val_loss: 3946.4587 - val_mae: 48.1615 - val_acc: 0.9624\n",
      "Epoch 2/50\n",
      "7710/7710 [==============================] - 5s 588us/step - loss: 982.5688 - mae: 18.7409 - acc: 0.9713 - val_loss: 134.8243 - val_mae: 7.5034 - val_acc: 0.9760\n",
      "Epoch 3/50\n",
      "7710/7710 [==============================] - 5s 599us/step - loss: 109.7859 - mae: 6.6884 - acc: 0.9757 - val_loss: 100.2411 - val_mae: 6.3630 - val_acc: 0.9771\n",
      "Epoch 4/50\n",
      "7710/7710 [==============================] - 5s 592us/step - loss: 93.9059 - mae: 6.1261 - acc: 0.9765 - val_loss: 89.4347 - val_mae: 5.9818 - val_acc: 0.9784\n",
      "Epoch 5/50\n",
      "7710/7710 [==============================] - 5s 618us/step - loss: 86.8402 - mae: 5.8874 - acc: 0.9776 - val_loss: 82.6455 - val_mae: 5.7326 - val_acc: 0.9791\n",
      "Epoch 6/50\n",
      "7710/7710 [==============================] - 5s 599us/step - loss: 82.1001 - mae: 5.7223 - acc: 0.9783 - val_loss: 79.5731 - val_mae: 5.6445 - val_acc: 0.9797\n",
      "Epoch 7/50\n",
      "7710/7710 [==============================] - 5s 595us/step - loss: 78.4995 - mae: 5.6030 - acc: 0.9790 - val_loss: 82.6572 - val_mae: 5.8450 - val_acc: 0.9795\n",
      "Epoch 8/50\n",
      "7710/7710 [==============================] - 4s 582us/step - loss: 75.7488 - mae: 5.5073 - acc: 0.9790 - val_loss: 74.1654 - val_mae: 5.4561 - val_acc: 0.9787\n",
      "Epoch 9/50\n",
      "7710/7710 [==============================] - 5s 593us/step - loss: 73.6616 - mae: 5.4331 - acc: 0.9796 - val_loss: 72.5866 - val_mae: 5.3816 - val_acc: 0.9805\n",
      "Epoch 10/50\n",
      "7710/7710 [==============================] - 4s 583us/step - loss: 71.3375 - mae: 5.3651 - acc: 0.9798 - val_loss: 72.2979 - val_mae: 5.4003 - val_acc: 0.9805\n",
      "Epoch 11/50\n",
      "7710/7710 [==============================] - 5s 594us/step - loss: 69.7330 - mae: 5.3137 - acc: 0.9799 - val_loss: 70.6872 - val_mae: 5.2924 - val_acc: 0.9814\n",
      "Epoch 12/50\n",
      "7710/7710 [==============================] - 5s 598us/step - loss: 68.1750 - mae: 5.2698 - acc: 0.9802 - val_loss: 68.6884 - val_mae: 5.2239 - val_acc: 0.9805\n",
      "Epoch 13/50\n",
      "7710/7710 [==============================] - 4s 571us/step - loss: 66.5559 - mae: 5.2239 - acc: 0.9804 - val_loss: 67.8776 - val_mae: 5.2133 - val_acc: 0.9813\n",
      "Epoch 14/50\n",
      "7710/7710 [==============================] - 4s 577us/step - loss: 65.3331 - mae: 5.1835 - acc: 0.9806 - val_loss: 66.8167 - val_mae: 5.1858 - val_acc: 0.9803\n",
      "Epoch 15/50\n",
      "7710/7710 [==============================] - 4s 573us/step - loss: 64.0550 - mae: 5.1519 - acc: 0.9806 - val_loss: 67.4862 - val_mae: 5.2374 - val_acc: 0.9815\n",
      "Epoch 16/50\n",
      "7710/7710 [==============================] - 4s 575us/step - loss: 63.1833 - mae: 5.1183 - acc: 0.9806 - val_loss: 65.3473 - val_mae: 5.1348 - val_acc: 0.9797\n",
      "Epoch 17/50\n",
      "7710/7710 [==============================] - 4s 582us/step - loss: 62.0845 - mae: 5.0853 - acc: 0.9806 - val_loss: 66.0108 - val_mae: 5.1874 - val_acc: 0.9810\n",
      "Epoch 18/50\n",
      "7710/7710 [==============================] - 5s 592us/step - loss: 61.3779 - mae: 5.0593 - acc: 0.9808 - val_loss: 64.3023 - val_mae: 5.0822 - val_acc: 0.9805\n",
      "Epoch 19/50\n",
      "7710/7710 [==============================] - 5s 601us/step - loss: 60.5679 - mae: 5.0368 - acc: 0.9811 - val_loss: 63.4414 - val_mae: 5.0631 - val_acc: 0.9814\n",
      "Epoch 20/50\n",
      "7710/7710 [==============================] - 5s 600us/step - loss: 59.7034 - mae: 5.0044 - acc: 0.9811 - val_loss: 62.5945 - val_mae: 5.0272 - val_acc: 0.9810\n",
      "Epoch 21/50\n",
      "7710/7710 [==============================] - 5s 594us/step - loss: 59.0291 - mae: 4.9822 - acc: 0.9813 - val_loss: 62.1418 - val_mae: 5.0157 - val_acc: 0.9808\n",
      "Epoch 22/50\n",
      "7710/7710 [==============================] - 5s 590us/step - loss: 58.0533 - mae: 4.9589 - acc: 0.9812 - val_loss: 62.0078 - val_mae: 5.0174 - val_acc: 0.9817\n",
      "Epoch 23/50\n",
      "7710/7710 [==============================] - 4s 550us/step - loss: 57.4474 - mae: 4.9447 - acc: 0.9816 - val_loss: 61.4870 - val_mae: 5.0102 - val_acc: 0.9810\n",
      "Epoch 24/50\n",
      "7710/7710 [==============================] - 4s 554us/step - loss: 56.6592 - mae: 4.9228 - acc: 0.9816 - val_loss: 60.8418 - val_mae: 4.9744 - val_acc: 0.9817\n",
      "Epoch 25/50\n",
      "7710/7710 [==============================] - 4s 542us/step - loss: 55.9896 - mae: 4.9032 - acc: 0.9817 - val_loss: 59.8794 - val_mae: 4.9593 - val_acc: 0.9822\n",
      "Epoch 26/50\n",
      "7710/7710 [==============================] - 4s 564us/step - loss: 55.1626 - mae: 4.8863 - acc: 0.9817 - val_loss: 59.2655 - val_mae: 4.9041 - val_acc: 0.9822\n",
      "Epoch 27/50\n",
      "7710/7710 [==============================] - 4s 555us/step - loss: 54.4994 - mae: 4.8702 - acc: 0.9816 - val_loss: 59.4096 - val_mae: 4.9432 - val_acc: 0.9817\n",
      "Epoch 28/50\n",
      "7710/7710 [==============================] - 4s 538us/step - loss: 53.8632 - mae: 4.8561 - acc: 0.9820 - val_loss: 59.1065 - val_mae: 4.9195 - val_acc: 0.9816\n",
      "Epoch 29/50\n",
      "7710/7710 [==============================] - 4s 524us/step - loss: 53.2934 - mae: 4.8378 - acc: 0.9820 - val_loss: 58.0731 - val_mae: 4.8829 - val_acc: 0.9824\n",
      "Epoch 30/50\n",
      "7710/7710 [==============================] - 4s 534us/step - loss: 52.8072 - mae: 4.8238 - acc: 0.9819 - val_loss: 57.4550 - val_mae: 4.8537 - val_acc: 0.9829\n",
      "Epoch 31/50\n",
      "7710/7710 [==============================] - 4s 536us/step - loss: 52.3160 - mae: 4.8100 - acc: 0.9822 - val_loss: 58.1926 - val_mae: 4.9128 - val_acc: 0.9812\n",
      "Epoch 32/50\n",
      "7710/7710 [==============================] - 4s 535us/step - loss: 51.7713 - mae: 4.7965 - acc: 0.9823 - val_loss: 58.0547 - val_mae: 4.9160 - val_acc: 0.9817\n",
      "Epoch 33/50\n",
      "7710/7710 [==============================] - 4s 560us/step - loss: 51.5679 - mae: 4.7838 - acc: 0.9822 - val_loss: 56.7542 - val_mae: 4.8398 - val_acc: 0.9826\n",
      "Epoch 34/50\n",
      "7710/7710 [==============================] - 4s 568us/step - loss: 51.0193 - mae: 4.7709 - acc: 0.9823 - val_loss: 56.4175 - val_mae: 4.8152 - val_acc: 0.9825\n",
      "Epoch 35/50\n",
      "7710/7710 [==============================] - 4s 560us/step - loss: 50.7590 - mae: 4.7597 - acc: 0.9824 - val_loss: 56.2639 - val_mae: 4.8118 - val_acc: 0.9827\n",
      "Epoch 36/50\n",
      "7710/7710 [==============================] - 4s 543us/step - loss: 50.7028 - mae: 4.7481 - acc: 0.9823 - val_loss: 56.1487 - val_mae: 4.7920 - val_acc: 0.9822\n",
      "Epoch 37/50\n",
      "7710/7710 [==============================] - 4s 553us/step - loss: 50.1438 - mae: 4.7341 - acc: 0.9824 - val_loss: 57.1377 - val_mae: 4.8672 - val_acc: 0.9827\n",
      "Epoch 38/50\n",
      "7710/7710 [==============================] - 4s 550us/step - loss: 49.7803 - mae: 4.7292 - acc: 0.9823 - val_loss: 55.4635 - val_mae: 4.7619 - val_acc: 0.9825\n",
      "Epoch 39/50\n",
      "7710/7710 [==============================] - 4s 549us/step - loss: 49.4926 - mae: 4.7194 - acc: 0.9822 - val_loss: 55.4772 - val_mae: 4.7702 - val_acc: 0.9827\n",
      "Epoch 40/50\n",
      "7710/7710 [==============================] - 4s 554us/step - loss: 49.2304 - mae: 4.7117 - acc: 0.9824 - val_loss: 55.3849 - val_mae: 4.7883 - val_acc: 0.9821\n",
      "Epoch 41/50\n",
      "7710/7710 [==============================] - 4s 549us/step - loss: 48.8288 - mae: 4.6973 - acc: 0.9823 - val_loss: 57.2063 - val_mae: 4.9206 - val_acc: 0.9830\n",
      "Epoch 42/50\n",
      "7710/7710 [==============================] - 4s 553us/step - loss: 48.6576 - mae: 4.6897 - acc: 0.9825 - val_loss: 54.7840 - val_mae: 4.7783 - val_acc: 0.9826\n",
      "Epoch 43/50\n",
      "7710/7710 [==============================] - 4s 550us/step - loss: 48.2357 - mae: 4.6818 - acc: 0.9825 - val_loss: 55.0686 - val_mae: 4.8069 - val_acc: 0.9831\n",
      "Epoch 44/50\n",
      "7710/7710 [==============================] - 4s 583us/step - loss: 47.9963 - mae: 4.6735 - acc: 0.9824 - val_loss: 53.6677 - val_mae: 4.7173 - val_acc: 0.9829\n",
      "Epoch 45/50\n",
      "7710/7710 [==============================] - 4s 578us/step - loss: 47.8517 - mae: 4.6619 - acc: 0.9827 - val_loss: 53.6511 - val_mae: 4.7328 - val_acc: 0.9826\n",
      "Epoch 46/50\n",
      "7710/7710 [==============================] - 5s 588us/step - loss: 47.5228 - mae: 4.6576 - acc: 0.9827 - val_loss: 53.3111 - val_mae: 4.7229 - val_acc: 0.9832\n",
      "Epoch 47/50\n",
      "7710/7710 [==============================] - 4s 577us/step - loss: 47.2418 - mae: 4.6497 - acc: 0.9826 - val_loss: 53.9495 - val_mae: 4.7352 - val_acc: 0.9822\n",
      "Epoch 48/50\n",
      "7710/7710 [==============================] - 4s 583us/step - loss: 47.1722 - mae: 4.6398 - acc: 0.9826 - val_loss: 54.0480 - val_mae: 4.7698 - val_acc: 0.9831\n",
      "Epoch 49/50\n",
      "7710/7710 [==============================] - 5s 586us/step - loss: 46.7477 - mae: 4.6345 - acc: 0.9826 - val_loss: 53.9695 - val_mae: 4.7459 - val_acc: 0.9830\n",
      "Epoch 50/50\n",
      "7710/7710 [==============================] - 5s 587us/step - loss: 46.5069 - mae: 4.6249 - acc: 0.9827 - val_loss: 52.6389 - val_mae: 4.7088 - val_acc: 0.9832\n",
      "2410/2410 [==============================] - 1s 289us/step\n",
      "7.347435658739749\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_drop_pmax)\n",
    "X_test_scaled = scaler.transform(X_test_drop_pmax)\n",
    "\n",
    "def build_model(hidden_units=64, activation='relu', optimizer='adam'):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(hidden_units, activation=activation, input_shape=(X_train_drop_pmax.shape[1],)),\n",
    "        layers.Dense(hidden_units, activation=activation),\n",
    "        layers.Dense(2)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(hidden_units=128, activation='sigmoid', optimizer='adam')\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "distance=np.mean(np.sqrt(np.sum((y_pred - y_test) ** 2, axis=1)))\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters tuning\n",
    "https://keras.io/guides/keras_tuner/getting_started/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras_tuner import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "    for i in range(hp.Int('num_layers', min_value=1, max_value=5, step=1)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=512, step=32),\n",
    "                               activation='sigmoid'))\n",
    "        model.add(layers.Dropout(rate=hp.Float('dropout_' + str(i), min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(layers.Dense(units=2))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.legacy.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')),\n",
    "                  loss='mse',\n",
    "                  metrics=['mae', 'acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,\n",
    "    executions_per_trial=3,\n",
    "    overwrite=True,\n",
    "    directory='my_tuning_directory',\n",
    "    project_name='my_first_tuning_project'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 22 Complete [00h 13m 39s]\n",
      "val_loss: 33.81149164835612\n",
      "\n",
      "Best val_loss So Far: 25.951648076375324\n",
      "Total elapsed time: 13h 36m 54s\n",
      "\n",
      "Search: Running Trial #23\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "1                 |4                 |num_layers\n",
      "384               |480               |units_0\n",
      "0.4               |0.3               |dropout_0\n",
      "0.00019167        |0.00018093        |learning_rate\n",
      "480               |96                |units_1\n",
      "0.3               |0                 |dropout_1\n",
      "96                |160               |units_2\n",
      "0.1               |0.1               |dropout_2\n",
      "192               |96                |units_3\n",
      "0.1               |0.1               |dropout_3\n",
      "64                |256               |units_4\n",
      "0.3               |0.4               |dropout_4\n",
      "\n",
      "Epoch 1/50\n",
      "9638/9638 [==============================] - 6s 578us/step - loss: 61189.4531 - mae: 205.5058 - acc: 0.5173 - val_loss: 12498.0029 - val_mae: 94.9019 - val_acc: 0.5841\n",
      "Epoch 2/50\n",
      "9638/9638 [==============================] - 5s 569us/step - loss: 8088.4419 - mae: 74.4416 - acc: 0.9314 - val_loss: 4032.1946 - val_mae: 50.3624 - val_acc: 0.9826\n",
      "Epoch 3/50\n",
      "9638/9638 [==============================] - 6s 577us/step - loss: 2548.7080 - mae: 38.7589 - acc: 0.9616 - val_loss: 752.5608 - val_mae: 18.1161 - val_acc: 0.9852\n",
      "Epoch 4/50\n",
      "9638/9638 [==============================] - 5s 567us/step - loss: 991.2672 - mae: 24.2292 - acc: 0.9537 - val_loss: 184.8312 - val_mae: 8.7495 - val_acc: 0.9828\n",
      "Epoch 5/50\n",
      "9638/9638 [==============================] - 5s 567us/step - loss: 764.6944 - mae: 21.4638 - acc: 0.9531 - val_loss: 104.3314 - val_mae: 6.6328 - val_acc: 0.9832\n",
      "Epoch 6/50\n",
      "9638/9638 [==============================] - 5s 570us/step - loss: 717.7277 - mae: 20.7940 - acc: 0.9541 - val_loss: 83.3184 - val_mae: 6.1053 - val_acc: 0.9802\n",
      "Epoch 7/50\n",
      "9638/9638 [==============================] - 6s 593us/step - loss: 690.1576 - mae: 20.3747 - acc: 0.9557 - val_loss: 74.3301 - val_mae: 5.7225 - val_acc: 0.9818\n",
      "Epoch 8/50\n",
      "9638/9638 [==============================] - 6s 593us/step - loss: 674.2143 - mae: 20.1332 - acc: 0.9574 - val_loss: 67.4493 - val_mae: 5.4053 - val_acc: 0.9822\n",
      "Epoch 9/50\n",
      "9638/9638 [==============================] - 6s 586us/step - loss: 660.9006 - mae: 19.9339 - acc: 0.9579 - val_loss: 66.4260 - val_mae: 5.4158 - val_acc: 0.9820\n",
      "Epoch 10/50\n",
      "9638/9638 [==============================] - 6s 575us/step - loss: 649.0895 - mae: 19.7666 - acc: 0.9583 - val_loss: 62.8537 - val_mae: 5.2707 - val_acc: 0.9836\n",
      "Epoch 11/50\n",
      "9638/9638 [==============================] - 6s 572us/step - loss: 636.9801 - mae: 19.5749 - acc: 0.9593 - val_loss: 61.1358 - val_mae: 5.2014 - val_acc: 0.9842\n",
      "Epoch 12/50\n",
      "9638/9638 [==============================] - 6s 576us/step - loss: 626.5999 - mae: 19.3907 - acc: 0.9604 - val_loss: 57.0661 - val_mae: 4.9491 - val_acc: 0.9844\n",
      "Epoch 13/50\n",
      "9638/9638 [==============================] - 6s 587us/step - loss: 619.2147 - mae: 19.2839 - acc: 0.9602 - val_loss: 60.3385 - val_mae: 5.2171 - val_acc: 0.9850\n",
      "Epoch 14/50\n",
      "9638/9638 [==============================] - 6s 585us/step - loss: 611.9331 - mae: 19.1755 - acc: 0.9613 - val_loss: 54.4713 - val_mae: 4.8648 - val_acc: 0.9853\n",
      "Epoch 15/50\n",
      "9638/9638 [==============================] - 6s 586us/step - loss: 603.6473 - mae: 19.0486 - acc: 0.9608 - val_loss: 53.9591 - val_mae: 4.8770 - val_acc: 0.9857\n",
      "Epoch 16/50\n",
      "9638/9638 [==============================] - 6s 587us/step - loss: 598.3725 - mae: 18.9469 - acc: 0.9620 - val_loss: 57.2472 - val_mae: 5.1130 - val_acc: 0.9842\n",
      "Epoch 17/50\n",
      "9638/9638 [==============================] - 6s 581us/step - loss: 593.4525 - mae: 18.8780 - acc: 0.9628 - val_loss: 58.1273 - val_mae: 5.1797 - val_acc: 0.9840\n",
      "Epoch 18/50\n",
      "9638/9638 [==============================] - 5s 569us/step - loss: 588.2714 - mae: 18.7968 - acc: 0.9630 - val_loss: 50.4724 - val_mae: 4.6671 - val_acc: 0.9810\n",
      "Epoch 19/50\n",
      "9638/9638 [==============================] - 6s 575us/step - loss: 587.4998 - mae: 18.7801 - acc: 0.9633 - val_loss: 52.8919 - val_mae: 4.8798 - val_acc: 0.9844\n",
      "Epoch 20/50\n",
      "9638/9638 [==============================] - 5s 570us/step - loss: 580.8984 - mae: 18.6613 - acc: 0.9632 - val_loss: 50.4114 - val_mae: 4.7152 - val_acc: 0.9830\n",
      "Epoch 21/50\n",
      "9638/9638 [==============================] - 5s 567us/step - loss: 578.6650 - mae: 18.6369 - acc: 0.9635 - val_loss: 51.8144 - val_mae: 4.8494 - val_acc: 0.9844\n",
      "Epoch 22/50\n",
      "9638/9638 [==============================] - 5s 569us/step - loss: 573.5431 - mae: 18.5403 - acc: 0.9637 - val_loss: 49.9706 - val_mae: 4.6972 - val_acc: 0.9821\n",
      "Epoch 23/50\n",
      "9638/9638 [==============================] - 5s 569us/step - loss: 569.8680 - mae: 18.5000 - acc: 0.9634 - val_loss: 49.1706 - val_mae: 4.6776 - val_acc: 0.9852\n",
      "Epoch 24/50\n",
      "9638/9638 [==============================] - 5s 565us/step - loss: 568.7261 - mae: 18.4830 - acc: 0.9638 - val_loss: 53.0798 - val_mae: 4.9232 - val_acc: 0.9829\n",
      "Epoch 25/50\n",
      "9638/9638 [==============================] - 6s 572us/step - loss: 567.6072 - mae: 18.4580 - acc: 0.9637 - val_loss: 50.1242 - val_mae: 4.7411 - val_acc: 0.9834\n",
      "Epoch 26/50\n",
      "9638/9638 [==============================] - 5s 569us/step - loss: 566.6176 - mae: 18.4352 - acc: 0.9636 - val_loss: 51.3691 - val_mae: 4.8522 - val_acc: 0.9850\n",
      "Epoch 27/50\n",
      "9638/9638 [==============================] - 6s 572us/step - loss: 561.8425 - mae: 18.3695 - acc: 0.9637 - val_loss: 54.0357 - val_mae: 5.0160 - val_acc: 0.9842\n",
      "Epoch 28/50\n",
      "9638/9638 [==============================] - 6s 583us/step - loss: 561.8582 - mae: 18.3444 - acc: 0.9639 - val_loss: 51.9595 - val_mae: 4.8470 - val_acc: 0.9834\n",
      "Epoch 29/50\n",
      "9638/9638 [==============================] - 6s 585us/step - loss: 556.3732 - mae: 18.2579 - acc: 0.9645 - val_loss: 53.5192 - val_mae: 5.0295 - val_acc: 0.9851\n",
      "Epoch 30/50\n",
      "9638/9638 [==============================] - 6s 574us/step - loss: 557.3759 - mae: 18.2610 - acc: 0.9638 - val_loss: 49.4818 - val_mae: 4.7249 - val_acc: 0.9854\n",
      "Epoch 31/50\n",
      "9638/9638 [==============================] - 5s 569us/step - loss: 556.2869 - mae: 18.2627 - acc: 0.9639 - val_loss: 48.7042 - val_mae: 4.7090 - val_acc: 0.9845\n",
      "Epoch 32/50\n",
      "9638/9638 [==============================] - 6s 573us/step - loss: 553.1873 - mae: 18.1988 - acc: 0.9645 - val_loss: 50.4711 - val_mae: 4.8387 - val_acc: 0.9839\n",
      "Epoch 33/50\n",
      "9638/9638 [==============================] - 6s 582us/step - loss: 552.4858 - mae: 18.1937 - acc: 0.9641 - val_loss: 50.0008 - val_mae: 4.7984 - val_acc: 0.9830\n",
      "Epoch 34/50\n",
      "9638/9638 [==============================] - 6s 586us/step - loss: 549.6129 - mae: 18.1423 - acc: 0.9644 - val_loss: 49.0423 - val_mae: 4.7310 - val_acc: 0.9848\n",
      "Epoch 35/50\n",
      "9638/9638 [==============================] - 6s 583us/step - loss: 550.6418 - mae: 18.1543 - acc: 0.9638 - val_loss: 47.6181 - val_mae: 4.6172 - val_acc: 0.9854\n",
      "Epoch 36/50\n",
      "7336/9638 [=====================>........] - ETA: 1s - loss: 551.4970 - mae: 18.1561 - acc: 0.9642"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:233\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n",
      "\u001b[1;32m    230\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n",
      "\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n",
      "\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n",
      "\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:273\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n",
      "\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n",
      "\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 273\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n",
      "\u001b[1;32m    274\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n",
      "\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:238\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n",
      "\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n",
      "\u001b[0;32m--> 238\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n",
      "\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n",
      "\u001b[1;32m    240\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[1;32m    241\u001b[0m     ):\n",
      "\u001b[1;32m    242\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n",
      "\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n",
      "\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n",
      "\u001b[1;32m    245\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n",
      "\u001b[1;32m    246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    253\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n",
      "\u001b[1;32m    254\u001b[0m         )\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n",
      "\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n",
      "\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_and_fit_model(trial, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n",
      "\u001b[1;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n",
      "\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n",
      "\u001b[1;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n",
      "\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n",
      "\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n",
      "\u001b[1;32m    127\u001b[0m \n",
      "\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n",
      "\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n",
      "\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n",
      "\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n",
      "\u001b[1;32m   1805\u001b[0m ):\n",
      "\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n",
      "\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n",
      "\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n",
      "\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n",
      "\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n",
      "\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n",
      "\u001b[1;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n",
      "\u001b[1;32m    870\u001b[0m   )\n",
      "\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n",
      "\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n",
      "\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:132\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n",
      "\u001b[1;32m    130\u001b[0m args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n",
      "\u001b[1;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "\u001b[0;32m--> 132\u001b[0m function \u001b[38;5;241m=\u001b[39m trace_function(\n",
      "\u001b[1;32m    133\u001b[0m     args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, tracing_options\u001b[38;5;241m=\u001b[39mtracing_options\n",
      "\u001b[1;32m    134\u001b[0m )\n",
      "\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n",
      "\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n",
      "\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n",
      "\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n",
      "\u001b[1;32m    179\u001b[0m       args, kwargs, tracing_options\n",
      "\u001b[1;32m    180\u001b[0m   )\n",
      "\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n",
      "\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:239\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n",
      "\u001b[1;32m    229\u001b[0m lookup_func_type, lookup_func_context \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m    230\u001b[0m     function_type_utils\u001b[38;5;241m.\u001b[39mmake_canonicalized_monomorphic_type(\n",
      "\u001b[1;32m    231\u001b[0m         args,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    235\u001b[0m     )\n",
      "\u001b[1;32m    236\u001b[0m )\n",
      "\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 239\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39mlookup(\n",
      "\u001b[1;32m    240\u001b[0m       lookup_func_type, current_func_context\n",
      "\u001b[1;32m    241\u001b[0m   )\n",
      "\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    243\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_cache.py:50\u001b[0m, in \u001b[0;36mFunctionCache.lookup\u001b[0;34m(self, function_type, context)\u001b[0m\n",
      "\u001b[1;32m     48\u001b[0m   dispatch_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_dict[context]\u001b[38;5;241m.\u001b[39mdispatch(function_type)\n",
      "\u001b[1;32m     49\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dispatch_type:\n",
      "\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary[(context, dispatch_type)]\n",
      "\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_type.py:456\u001b[0m, in \u001b[0;36mFunctionType.__hash__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n",
      "\u001b[0;32m--> 456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m((\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mitems()), \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptures\u001b[38;5;241m.\u001b[39mitems())))\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/tensor.py:895\u001b[0m, in \u001b[0;36mDenseSpec.__hash__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;32m--> 895\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype))\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/tensor_shape.py:1508\u001b[0m, in \u001b[0;36mTensorShape.__hash__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[1;32m   1506\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;241m==\u001b[39m other\u001b[38;5;241m.\u001b[39m_dims\n",
      "\u001b[0;32m-> 1508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m   1509\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims)\n",
      "\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__reduce__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=50, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to limited computing resources, manual modification and experimental methods are used to adjust parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I ran this part on kaggle, so there is no output\n",
    "from tensorflow.keras.optimizers import schedules\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def build_model(activation='sigmoid', optimizer='adam'):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(512, activation=activation, input_dim=X_train.shape[1]),\n",
    "        #layers.Dropout(0.2),\n",
    "        layers.Dense(512, activation=activation),\n",
    "        #layers.Dropout(0.2),\n",
    "        layers.Dense(2, activation=None)\n",
    "    ])\n",
    "    \n",
    "    learning_rate_schedule = schedules.ExponentialDecay(initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.9)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate_schedule)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[\"mae\", \"acc\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "distance=np.mean(np.sqrt(np.sum((y_pred - y_test) ** 2, axis=1)))\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_df_scaled = scaler.transform(eva_df)\n",
    "\n",
    "eva_y=model.predict(eva_df_scaled)\n",
    "\n",
    "df = pd.DataFrame(eva_y, columns=['Id', 'Predicted'])\n",
    "df['Predicted'] = df.apply(lambda row: f\"{row['Id']:.1f}|{row['Predicted']:.1f}\", axis=1)\n",
    "df['Id'] = df.index\n",
    "df.to_csv('tensor_output.csv', index=False, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
